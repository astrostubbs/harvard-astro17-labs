{"cells":[{"cell_type":"markdown","id":"f283ec46","metadata":{"id":"f283ec46"},"source":["# Interactive Exploration of Fitting Data to Models:<br> Gaussian Distributions, Reduced Chi-Squared, Linear vs. Non-Linear Fits"]},{"cell_type":"markdown","id":"0159150f","metadata":{"id":"0159150f"},"source":["## Introduction"]},{"cell_type":"markdown","id":"3d84ddf1","metadata":{"id":"3d84ddf1"},"source":["This notebook was created by [Jupyter AI](https://github.com/jupyterlab/jupyter-ai) with the following prompt:\n","\n","> /generate act as an expert in hand-on STEM learning. I want you to generate an interactive Jupyter notebook that does the following:\n","\n","introduce the concept of a Gaussian distribution as a model for an observation and associated uncertainty,\n","Ask students to evaluate how likely is is that 'truth' lies more than one sigma away from the observation,\n","produces a random set of 10 data points that lie along a line, with associated noise. Plots them with error bars. make the x axis range from 1 to 10.\n","Make an interactive cell that has sliders to adjust slope and intercept of a fit, and computes the reduced chi-squared for the fit. Add a tutorial cell that talks about minimzing negative log likelihood.\n","include a 3-d plot of the reduced chi-squared surface (which should be parabolic) and show the chosen value as it evolves."]},{"cell_type":"markdown","id":"f1de16f5-c307-4d5d-b825-90ab2cf04ca4","metadata":{"id":"f1de16f5-c307-4d5d-b825-90ab2cf04ca4"},"source":["--------------------------------"]},{"cell_type":"markdown","id":"549f95cd","metadata":{"id":"549f95cd"},"source":["This Jupyter notebook offers an immersive exploration into the world of Gaussian distributions, uncertainty quantification, and linear fitting, centering on the reduced chi-squared statistic. It begins with an introduction to Gaussian distributions, highlighting the roles of mean and standard deviation as indicators of central tendency and spread, respectively, and representing uncertainty through standard deviation. Students are then guided to calculate the probability of the true value lying within one standard deviation from the mean using the Gaussian cumulative distribution function. Further, the notebook generates a set of 10 random data points, akin to a line with Gaussian noise, and uses Python's numpy and matplotlib libraries to visualize these points with error bars across an x-axis ranging from 1 to 10. An interactive component is included, featuring Jupyter widgets that allow users to adjust the slope and intercept of a fitted line and calculate the reduced chi-squared statistic for this fit. Additionally, a tutorial demonstrates the concept of maximum likelihood estimation (MLE), emphasizing how minimizing the negative log likelihood relates to effective data fitting and model selection. The notebook culminates with a 3D plot showcasing the reduced chi-squared surface over varying slope and intercept values using matplotlib's mplot3d, illustrating the dynamics of chosen fit values with evolving markers on the surface."]},{"cell_type":"markdown","id":"4ea34876-c28b-49f3-a920-1a7b80364026","metadata":{"id":"4ea34876-c28b-49f3-a920-1a7b80364026"},"source":["## Likelihood of the Truth Being a Distance \\(X\\) from the Measured Value\n","\n","In scenarios where measurements have Gaussian (normal) distributed errors, it's important to understand the likelihood of the \"true\" value being a certain distance \\(X\\) away from the measured value. This knowledge is vital for error estimation and confidence analysis in experimental data.\n","\n","### Gaussian Distribution with Non-zero Mean\n","\n","We're going to make a model of the uncertainties associated with a measurement as being a Gaussian with mean $\\mu$ and standard deviation $\\sigma$. The probability density function (PDF) of the Gaussian distribution gives the likelihood of obtaining a given measurement x, and is:\n","\n","$$\n","f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)\n","$$\n","\n","where \\(x\\) is the observed measurement.\n","\n","The mean $\\mu$ is our estimator of some 'true' underlying value, and $\\sigma$ reports the 'noise' associated with a single measurement.\n","\n","### Likelihood of Truth being a Distance X from $\\mu$.\n","\n","The likelihood that the true value is within a distance \\(X\\) from the measured value involves looking at the probability that the true value falls within an interval around the measurement. This can be calculated using the cumulative distribution function (CDF) approach for a range from \\(-X\\) to \\(X\\):\n","\n","$$\n","P(-X < \\Delta x < X) = \\int_{\\mu - X}^{\\mu + X} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right) \\, dx\n","$$\n","\n","where:\n","$\\Delta x = x_{\\text{true}} - x_{\\text{measured}}$\n","\n","### Interpretation\n","\n","- **Distribution Centered on $\\mu$:** The mean of the distribution $\\mu$ represents the central tendency of the measurements, and shifts the center of our interval of integration.\n","- **Confidence Intervals:** This analysis can inform confidence intervals around a measured value, providing insights into the precision and accuracy of measurements by quantifying the probability that the true value lies within a certain range of the measured value.\n"]},{"cell_type":"markdown","id":"be6bb793-0878-4db7-9741-8fc69f13da9f","metadata":{"id":"be6bb793-0878-4db7-9741-8fc69f13da9f"},"source":["## Interactive demonstration\n","\n","The cell below lets you select a mean $\\mu$ and standard deviation $\\sigma$. Think of this as corresponding to a data point and its associated uncertainty.\n","You can also use a slider to choose a 'truth' value T. The cell will plot a Gaussian probability distribution (PDF) and will report the likelihood of truth being that far off from $\\mu$.  "]},{"cell_type":"code","execution_count":null,"id":"c010e493-a77e-48fe-a9ae-3d4f9a88457d","metadata":{"id":"c010e493-a77e-48fe-a9ae-3d4f9a88457d"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import norm\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","def plot_pdf(mean, std, x_value):\n","    # Create the x values\n","    x = np.linspace(mean - 4*std, mean + 4*std, 1000)\n","\n","    # Calculate the PDF values\n","    pdf_values = norm.pdf(x, mean, std)\n","\n","    # Plot the PDF\n","    plt.figure(figsize=(8, 5))\n","    plt.plot(x, pdf_values, label='PDF', color='blue')\n","    plt.axvline(x=x_value, color='red', linestyle='--', label=f'Data Point: {x_value:.2f}')\n","    plt.title('Gaussian Distribution PDF')\n","    plt.xlabel('X')\n","    plt.ylabel('Probability Density')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","    # Calculate the discrepancy (z-score)\n","    z_score = np.abs(x_value - mean) / std\n","\n","    # Calculate the range bounds\n","    boundary = np.abs(x_value - mean)\n","\n","    # Calculate the likelihood of being more discrepant (two-tailed)\n","    prob_within_boundaries = norm.cdf(mean + boundary, mean, std) - norm.cdf(mean - boundary, mean, std)\n","    discrepant_prob = 1 - prob_within_boundaries\n","\n","    print(f\"Data point {x_value:.2f} is {z_score:.2f} sigma away from the mean\")\n","    print(f\"Likelihood of measurement being more extreme than {z_score:.2f} sigma: {discrepant_prob:.6f}\")\n","\n","# Create widgets for mean, std, and x_value\n","mean_widget = widgets.FloatSlider(value=0, min=-10, max=10, step=0.1, description='Mean')\n","std_widget = widgets.FloatSlider(value=1, min=0.1, max=5, step=0.1, description='Std Dev')\n","x_value_widget = widgets.FloatSlider(value=0, min=-10, max=10, step=0.05, description='T Value')\n","\n","# Use the interactive function to link the plot to the sliders\n","interactive_plot = widgets.interactive(plot_pdf, mean=mean_widget, std=std_widget, x_value=x_value_widget)\n","\n","# Display the interactive plot\n","display(interactive_plot)"]},{"cell_type":"markdown","id":"77d74391-08ff-45c7-b190-a1eb5df98843","metadata":{"id":"77d74391-08ff-45c7-b190-a1eb5df98843"},"source":["## Fitting Model Parameters with Uncertainties.\n","\n","With this in hand, we can now move on to finding the best-fit parameters for a model for a given a data set with values and associated uncertainties. <br>\n","We'll start by making a random data set, and we'll do a manual and automated linear fit to the data points."]},{"cell_type":"markdown","id":"45ea32d5","metadata":{"id":"45ea32d5"},"source":["## Generate Random Data and Plot with Error Bars"]},{"cell_type":"code","execution_count":null,"id":"6bae7eba","metadata":{"id":"6bae7eba"},"outputs":[],"source":["np.random.seed(42) # this initiates a random number generator"]},{"cell_type":"code","execution_count":null,"id":"dc9c9394","metadata":{"id":"dc9c9394"},"outputs":[],"source":["# pick some parameters\n","true_slope = 2.0\n","true_intercept = 5.0\n","num_points = 10"]},{"cell_type":"code","execution_count":null,"id":"35724acc","metadata":{"id":"35724acc"},"outputs":[],"source":["# make x, y, and uncertainty vectors\n","\n","x_values = np.arange(1, num_points + 1) # sets up an array ot 10 equally spaced x values\n","true_y_values = true_slope * x_values + true_intercept\n","\n","# leave this commented out until later on please\n","# true_y_values = true_slope * x_values + true_intercept + 0.5*x_data*x_data # makes y values according to parameters above\n","\n","noise = np.random.normal(0, 1, num_points) # create Gaussian noise vector with sigma=1 and mean 0.\n","y_values = true_y_values + noise # add noise to the data\n","\n","y_errors = np.ones(num_points) # create a vector of measurement uncertainties, with sigma = 1.\n"]},{"cell_type":"code","execution_count":null,"id":"83c215e6","metadata":{"id":"83c215e6"},"outputs":[],"source":["# It's ALWAYS a good idea to plot your data, so let's do that.\n","\n","plt.errorbar(x_values, y_values, yerr=y_errors, fmt='o')\n","plt.xlabel('X-axis')\n","plt.ylabel('Y-axis')\n","plt.title('Simulated Noisy Data Points with Error Bars')\n","plt.grid('on')\n","plt.show()"]},{"cell_type":"markdown","id":"2415dd31-75b6-49de-bff4-21632ce3163f","metadata":{"id":"2415dd31-75b6-49de-bff4-21632ce3163f"},"source":["## Least Squares Fitting and Its Connection to Negative Log Likelihood\n","\n","In statistical modeling, especially when dealing with Gaussian or normally distributed errors, the process of fitting a model to data often involves minimizing a specific cost function. This approach can be linked to both maximum likelihood estimation and the minimization of the reduced chi-squared statistic.\n","\n","### Gaussian Assumption\n","\n","If the errors in the measurements are assumed to be Gaussian and the measurements are independent (uncorrelated), then the likelihood $\\mathcal{L}$ of observing the data set given a model is the product of the probability per data point;\n","\n","$$\n","L = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\exp\\left(-\\frac{(y_i - y_{\\text{model}, i})^2}{2 \\sigma_i^2} \\right),\n","$$\n","\n","where:\n","- $y_i$ are the observed data points,\n","- $y_{\\text{model}, i}$ are the model predictions,\n","- $\\sigma_i$ are the standard deviations of the measurement errors,\n","- $N$ is the number of data points.\n","\n","If we want to find a mathematical model that is a best-fit to the data, we seek to maximize this likelihood.   \n","\n","### Negative Log Likelihood\n","\n","To simplify the calculation, we often work with the negative log likelihood:\n","\n","$$\n","-\\log{L} = \\sum_{i=1}^{N} \\frac{(y_i - y_{\\text{model}, i})^2}{2 \\sigma_i^2} + \\text{constant}\n","$$\n","\n","Minimizing the negative log likelihood is equivalent to minimizing the sum of squared, normalized residuals, commonly known as the least squares criterion:\n","\n","$$\n","\\sum_{i=1}^{N} \\frac{(y_i - y_{\\text{model}, i})^2}{\\sigma_i^2}\n","$$\n","\n","### Connection to Reduced Chi-Squared\n","\n","The least squares approach leads directly to the definition of the chi-squared statistic:\n","\n","$$\n","\\chi^2 = \\sum_{i=1}^{N} \\frac{(y_i - y_{\\text{model}, i})^2}{\\sigma_i^2}\n","$$\n","\n","In practice, the reduced chi-squared statistic is used to account for the degrees of freedom in the model, providing a normalized measure:\n","\n","$$\n","\\chi^2_{\\text{reduced}} = \\frac{\\chi^2}{\\text{dof}}\n","$$\n","\n","where:\n","- $\\text{dof} = N - p$ is the number of degrees of freedom,\n","- $p$ is the number of parameters in the model.\n","\n","### Interpretation\n","\n","- **Likelihood Maximization:** By minimizing the negative log likelihood, we effectively maximize the likelihood function, leading to the best-fit parameters under Gaussian assumptions.\n","- **Goodness of Fit:** The reduced chi-squared provides a measure of how well the model fits the data relative to the expected statistical variation. A value close to 1 usually indicates a good fit, assuming the model is correct and the errors are well estimated.\n","\n","This relationship between least squares fitting, likelihood estimation, and chi-squared statistics is a fundamental concept in statistical regression analysis, especially in the context of Gaussian-distributed measurement errors."]},{"cell_type":"markdown","id":"72ac7c18-e609-4228-89f7-103b34b35e4e","metadata":{"id":"72ac7c18-e609-4228-89f7-103b34b35e4e"},"source":["### What this interactive $\\chi_{\\rm{reduced}}^2$ surface shows\n","\n","This section visualizes the **reduced chi-squared ($\\chi_{\\rm{reduced}}^2$)** surface for a simple Gaussian model, as a function of the linear model's parameters (slope and intercept).  \n","The surface plot helps you see where the fit is a *better* (lower $\\chi_{\\rm{reduced}}^2$) or *worse* (higher $\\chi_{\\rm{reduced}}^2$) description of the data, and how parameters may be correlated.\n","\n","**How the plot is built:**\n","\n","1. A grid over the chosen parameter pair (slope and intercept) is created.\n","2. For each grid point, the model is evaluated and a χ² value is computed against the current dataset.\n","3. The resulting 2D array of χ² values is rendered as a **3D surface**: the height is $\\chi_{\\rm{reduced}}^2$, letting you see how well the model fits the data for different combinations of slope and intercept.  A marker on the surface shows the current combination of parameters.\n","\n","**Move the slope and intercept sliders to find the combination of parameters that minimizes $\\chi_{\\rm{reduced}}^2$.**  What values do you find yield the best fit?"]},{"cell_type":"code","execution_count":null,"id":"5588f91d","metadata":{"id":"5588f91d"},"outputs":[],"source":["from ipywidgets import interact, FloatSlider"]},{"cell_type":"code","execution_count":null,"id":"1d83c42c","metadata":{"id":"1d83c42c"},"outputs":[],"source":["# Sample data: x values and corresponding y values with associated uncertainties\n","x_data = x_values\n","y_data = y_values"]},{"cell_type":"code","execution_count":null,"id":"bee2f604-33c7-4591-a2dd-c0c3a97f629b","metadata":{"id":"bee2f604-33c7-4591-a2dd-c0c3a97f629b"},"outputs":[],"source":["# optional centered of x and y data about their respective means\n","centered = 0\n","if centered==1:\n","        x_data=x_data-np.mean(x_data)\n","        y_data=y_data-np.mean(y_data)\n","\n","# optional large offset\n","# x_data=x_data+10"]},{"cell_type":"code","execution_count":null,"id":"6081891c-b5e8-4c0f-840e-df7c3dc79210","metadata":{"id":"6081891c-b5e8-4c0f-840e-df7c3dc79210"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# Function to calculate reduced chi-squared\n","def reduced_chi_squared(y_obs, y_exp, errors, num_params):\n","    \"\"\"\n","    Calculate the reduced chi-squared statistic.\n","\n","    y_obs: Observed data\n","    y_exp: Expected data (model predictions)\n","    errors: Errors in the observed data\n","    num_params: Number of fitted parameters\n","\n","    Returns: Reduced chi-squared value\n","    \"\"\"\n","    chi_squared = np.sum(((y_obs - y_exp) / errors) ** 2)\n","    degrees_of_freedom = len(y_obs) - num_params\n","    return chi_squared / degrees_of_freedom\n","\n","# Interactive function to plot both 2D and 3D\n","def interactive_fit(slope, intercept):\n","\n","    y_model = slope * x_data + intercept\n","    red_chi_squared = reduced_chi_squared(y_data, y_model, y_errors, num_params=2)\n","\n","    # Create subplots side by side\n","    fig = plt.figure(figsize=(15, 6))\n","\n","    # Plotting the 2D fit\n","    ax1 = fig.add_subplot(121)\n","    ax1.errorbar(x_data, y_data, yerr=y_errors, fmt='o', label='Data with errors')\n","    ax1.plot(x_data, y_model, label=f'Fit: y = {slope:.2f}x + {intercept:.2f}')\n","    ax1.set_xlabel('x')\n","    ax1.set_ylabel('y')\n","    ax1.set_title(f'Linear Fit with Reduced Chi-Squared: {red_chi_squared:.2f}')\n","    ax1.legend()\n","    ax1.grid(True)\n","\n","    # Define meshgrid for slope and intercept values\n","    slope_range = np.linspace(-5, 5, 100)  # Match slider range\n","    intercept_range = np.linspace(-20, 20, 100)  # Match slider range\n","    slope_grid, intercept_grid = np.meshgrid(slope_range, intercept_range)\n","\n","    # Calculate the chi-squared surface\n","    chi_squared_surface = np.array([\n","        reduced_chi_squared(y_data, m * x_data + b, y_errors, num_params=2)\n","        for m, b in zip(np.ravel(slope_grid), np.ravel(intercept_grid))\n","    ]).reshape(slope_grid.shape)\n","\n","    # 3D surface plot\n","    ax2 = fig.add_subplot(122, projection='3d')\n","    surf = ax2.plot_surface(slope_grid, intercept_grid, np.log10(chi_squared_surface), cmap='viridis', alpha=0.8)\n","    ax2.scatter(slope, intercept, np.log10(red_chi_squared), color='red', s=100, label='Trial point',zorder=100,alpha=1)\n","    ax2.set_xlabel('Slope')\n","    ax2.set_ylabel('Intercept')\n","    ax2.set_zlabel('log Reduced Chi-Squared')\n","    ax2.set_xlim([-5, 5])\n","    ax2.set_ylim([-20, 20])\n","    ax2.set_title('log Reduced Chi-Squared Surface')\n","    ax2.legend()\n","    fig.colorbar(surf, ax=ax2, shrink=0.5, aspect=5)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Use interactive to adjust the slope and intercept\n","interact(interactive_fit, slope=(-5.0, 5.0, 0.01), intercept=(-20.0, 20.0, 0.01))"]},{"cell_type":"markdown","id":"9008c35e-1753-42b6-85d5-5f64e9ef10ba","metadata":{"id":"9008c35e-1753-42b6-85d5-5f64e9ef10ba"},"source":["### Moving from ``chi-by-eye'' to an automated fit: linear regression with uncertainties\n","\n","Now that we’ve explored the general behavior of this $\\chi_{\\rm{reduced}}^2$ surface, let’s apply these ideas to a **real curve fitting example**.  \n","We will fit a straight line\n","\n","$$\n","y = m x + b\n","$$\n","\n","to the dataset, taking into account the uncertainties on each data point.\n","\n","**What happens in this cell:**\n","\n","1. We define a simple linear model and use `scipy.optimize.curve_fit` to estimate the best-fit slope \\(m\\) and intercept \\(b\\).\n","2. The uncertainties on these parameters are extracted from the covariance matrix of the fit.\n","3. The data are plotted with error bars, along with the best-fit line.\n","4. We compute the **residuals** (data – model), the overall χ² statistic, and the **reduced χ²** to evaluate the quality of the fit.\n","\n","**Why this matters:**\n","\n","- If the reduced χ² is close to 1, it suggests the model and error estimates are consistent with the data.\n","- Larger values (≫1) indicate the model is not capturing the data trends or the error bars are too small.\n","- Smaller values (≪1) suggest the errors may be overestimated or the model is overfitting.\n","\n","This provides a concrete link between the $\\chi_{\\rm{reduced}}^2$ surface we visualized above and the practical task of fitting a model to experimental data.\n","\n","**How close is this best-fit model to the parameters you selected by interacting with the chi-squared surface?**\n"]},{"cell_type":"code","execution_count":null,"id":"0604d9ca-e164-4a93-b8e1-25d4795ed975","metadata":{"id":"0604d9ca-e164-4a93-b8e1-25d4795ed975"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.optimize import curve_fit\n","\n","# Define a linear function for fitting\n","def linear(x, m, b):\n","    return m * x + b\n","\n","# Perform the curve fit\n","popt, pcov = curve_fit(linear, x_data, y_data, sigma=y_errors, absolute_sigma=True)\n","\n","# Extract fit parameters and their uncertainties\n","m_best_fit, b_best_fit = popt\n","m_uncertainty, b_uncertainty = np.sqrt(np.diag(pcov))\n","\n","# Plot data\n","plt.errorbar(x_data, y_data, yerr=y_errors, fmt='o', label='Data with errors', color='blue')\n","x_fit = np.linspace(min(x_data) - 1, max(x_data) + 1, 100)\n","y_fit = linear(x_fit, m_best_fit, b_best_fit)\n","plt.plot(x_fit, y_fit, '-', label=f'Fit: y = ({m_best_fit:.2f} ± {m_uncertainty:.2f})x + ({b_best_fit:.2f} ± {b_uncertainty:.2f})', color='red')\n","\n","# Add plot legend\n","plt.legend(loc='best')\n","\n","# Add plot labels\n","plt.xlabel('X Data')\n","plt.ylabel('Y Data')\n","plt.title('Linear Fit with Uncertainties')\n","\n","# Add fit equation in a box\n","fit_eq = (f\"y = ({m_best_fit:.2f} ± {m_uncertainty:.2f})x + \"\n","          f\"({b_best_fit:.2f} ± {b_uncertainty:.2f})\")\n","plt.annotate(fit_eq, xy=(0.05, 0.95), xycoords='axes fraction',\n","             fontsize=10, ha='left', va='top',\n","             bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='gray', facecolor='lightyellow', alpha=0.5))\n","\n","plt.grid(True)\n","plt.show()\n","\n","# Calculate the residuals and chi-squared\n","residuals = y_data - linear(x_data, *popt)\n","chi_squared = np.sum((residuals / y_errors) ** 2)\n","\n","# Calculate degrees of freedom\n","dof = len(y_data) - len(popt)  # Number of data points minus number of fit parameters\n","\n","# Calculate reduced chi-squared\n","reduced_chi_squared = chi_squared / dof\n","\n","\n","# Print the fit results\n","print(f\"Slope (m) = {m_best_fit:.2f} ± {m_uncertainty:.2f}\")\n","print(f\"Intercept (b) = {b_best_fit:.2f} ± {b_uncertainty:.2f}\")\n","print(f\"Reduced Chi-Squared: {reduced_chi_squared:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"d643b09e-7a63-4f09-8d79-8047dcb6b1bf","metadata":{"id":"d643b09e-7a63-4f09-8d79-8047dcb6b1bf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"4e3af639-d388-483b-8550-6263da8a4403","metadata":{"id":"4e3af639-d388-483b-8550-6263da8a4403"},"outputs":[],"source":["# always a good idea to plot the residuals of the fit\n","\n","# Plot data\n","plt.errorbar(x_data, residuals, yerr=y_errors, fmt='o', label='Residuals', color='blue')\n","\n","# Add plot labels\n","plt.xlabel('X')\n","plt.ylabel('Residuals')\n","plt.title('Fit Residuals with Uncertainties')\n","\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","id":"673940fa-d28b-4e18-bdbb-9663b4cb212c","metadata":{"id":"673940fa-d28b-4e18-bdbb-9663b4cb212c"},"source":["# Linear vs. Non-Linear Fitting\n","\n","## Linear Fitting\n","\n","**Definition:**  \n","Linear fitting approximates data where model is linear in the fit parameters. One simple example is $y(x) = mx + b$, where:\n","- $ m $ is the slope, and\n","- $ b $ is the y-intercept.\n","\n","Another example of a linear fit is $y(x) = A \\sin(x) + B \\cos(x)$, where A and B are fit parameters.\n","\n","**Characteristics:**\n","- **Simple & Direct:** The relationship between variables is straightforward.\n","- **Analytical Solutions:** Solving a linear fit involves straightforward algebra and matrix algebra, leading to unique solutions. The $\\chi^2$ surface is simple.\n","- **Efficient:** Linear regression is computationally simple and fast, making it practical even for large datasets.\n","\n","**Applications:**  \n","- Used when data sets exhibit a clear linear trend.\n","- Widely employed in fields like economics (e.g., forecasting), biology (e.g., determining growth rates), and physics (e.g., relationships following Newtonian mechanics).\n","\n","\n","## Non-Linear Fitting\n","\n","**Definition:**  \n","Non-linear fitting is necessary when relationships between variables are more complex and cannot be captured with a straight line. Examples include exponential growth, power laws, sigmoidal curves, etc.\n","\n","**Characteristics:**\n","- **Complex Models:** Non-linear equations often have forms like \\( y = ae^{bx} \\), \\( y = ax^b \\), or more complex polynomial or sinusoidal functions.\n","- **Iterative Solutions:** Non-linear fitting requires iterative numerical algorithms (like Gauss-Newton or Levenberg-Marquardt) due to the absence of simple algebraic solutions.\n","- **Initial Guesses:** Requires initial parameter estimates to start the iterative process, which influence the convergence and outcome.\n","\n","**Pitfalls & Challenges:**\n","- **Convergence Issues:** Algorithmic convergence is not guaranteed; poor initial guesses can lead to non-convergence or convergence to local minima instead of a global solution.\n","- **Computational Intensity:** The iterative nature can be computationally expensive, especially for complex models or large datasets.\n","- **Sensitivity & Instability:** Non-linear models can be sensitive to small changes in data, leading to instability in estimates, especially if data quality is poor or models are overparametrized.\n","- **Overfitting:** Adding too many parameters can lead to modeling noise rather than the underlying trend, reducing predictive power on new data.\n","\n","## Conclusion\n","\n","The choice between linear and non-linear fitting should depend on:\n","- **Nature of Data:** Clearly linear relationships can use linear regression, while complex curves require non-linear methods.\n","- **Predictive Goal:** Consider whether you need straightforward predictions from clear trends or need to capture intricate patterns.\n","- **Model Simplicity vs. Accuracy:** Balance overfitting risks by choosing a model complex enough to capture significant patterns without excessively fitting noisy data points.\n","\n","Understanding these distinctions helps ensure the selected model appropriately represents the data, leveraging either the strengths of linear simplicity or non-linear flexibility, while remaining cautious of their respective pitfalls and challenges."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}