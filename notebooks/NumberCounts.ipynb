{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Photometric catalogs - Part 1: Star-Galaxy Discrimination and galaxy number Counts.\n",
        "\n",
        "C. Stubbs, Feb 14, 2025\n",
        "Andrés A. Plazas Malagón, Sep. 26, 2025,\n",
        "With assistance from ChatGPT and Gemini.\n",
        "\n",
        "### **Overview**\n",
        "\n",
        "This is the first of a pair of notebooks that provide a database of **photometry, size measurements, and photometric uncertainties** for sources around the galaxy clusters **Abell 1942** and **Abell 360**.  \n",
        "\n",
        "In this first notebook, we'll use data from the Sloan Digital Sky Survey (SDSS) from a galaxy cluster, Abell 1942, to learn how to separate galaxies from stars, and how to use galaxy counts as a function of brightness to test cosmological predictions.  \n",
        "\n",
        "In the second notebook, we'll use data from the Vera C. Rubin observatory from another cluster, Abell 360, to select red galaxies that are likely cluster members.  \n",
        "\n",
        "\n",
        "Here is a link to an image of Abell cluster 1492 in the DESI viewer. :\n",
        "https://www.legacysurvey.org/viewer#abell%201492\n",
        "\n",
        "(type A1942 in the search bar and then zoom in a bit).\n",
        "\n",
        "\n",
        "### **Learning Objectives**\n",
        "\n",
        "After working through this notebook you will have a good feel for how astronomers work with catalogued datasets that contain parametric descriptors for celestial objects. We start with broadband images of the sky taken in different optical and infrared passbands. Software is used to identify and characterize sources, and merge the detections into assigned attributes of distinct objects.\n",
        "\n",
        "You'll learn how to discriminate between stars and galaxies, using shape and flux information.\n",
        "\n",
        "You'll have the chance to fit models to data, and to compare cosmological predictions with observations.\n",
        "\n",
        "You'll learn how to select out a subset of objects from a parent catalog, and make a color-magnitude diagram for galaxies (!). You'll then play around with selecting a best-size aperture within which to select cluster members, and use their mean color to estimate the redshift of Abell 1942.\n",
        "\n",
        "### **Submitting Your Work**  \n",
        "\n",
        "As you work through this, you'll see places where you're asked to answer questions or to enter data. This is done by editing the appropriate text cell.\n",
        "\n",
        "You'll upload your version of this finished notebook to Canvas,\n",
        "with the file name\n",
        "\n",
        "***ClusterLRG_LastFirst_LastFirst.ipynb***\n",
        "\n",
        "where the LastFirst names are the team members.\n",
        "Go ahead and save this now, in accord with that naming convention.\n",
        "\n",
        "### **Steps in This Notebook**\n",
        "0. Rename and save this notebook in accord with the naming convention described above.\n",
        "1. Download SDSS photometric data, including positions, magnitudes, and associated uncertainties, for a roughly 1 degree x 1 degree patch of the sky. Transfer the file to your Google Drive.\n",
        "2. Select out catalog objects that satisfy a cut in signal to noise ratio, SNR.\n",
        "3. Separate the galaxies from the stars.\n",
        "4. Make a plot of number counts of high SNR galaxies, and compare to predictions. Woah.\n",
        "5. Devise selection criteria to pick out the red elliptical galaxies.\n",
        "6. Estimate the cluster redshift using the color of cluster members.\n",
        "7. Double-check that you've filled in the notebook with your work.\n",
        "8. Make sure you filled out the Acknowledgements panel.\n",
        "9. Save a final version with appropriate naming convention.\n",
        "10. Submit on Canvas in advance of deadline.\n",
        "11. Relax and celebrate.\n",
        "\n",
        "### Tips and Hints\n",
        "\n",
        "We'll make liberal use of the Gemini AI agent that is embedded in Colab. Make sure you understand the difference between the genric chat bot you invoke with the icon at the top right, vs. the one that interacts more directly with code cells.\n",
        "\n",
        "In some of the code cells I've left in the prompt that was used to intially create the cell. In many cases it was lightly edited after that initial creation, though.\n",
        "\n",
        "Remember you can click on the three dots on the right of a code cell and have it \"Explain Code\" to you.\n",
        "\n",
        "Places we are seeking your input are flagged like this:\n",
        "\n",
        "<font color=\"red\"> **your input needed here** </font>\n",
        "\n",
        "### **Why SDSS?**\n",
        "*The Sloan Digital SKy Survey (SDSS)* was the first modern wide-field, multi-band imaging plus spectroscopy survey.\n",
        "Photometric and spectroscopic catalogs are readily available, and it's a good starting point for us.  \n",
        "\n",
        "Open the Gemini chat window with the icon in the top right of the browser interface. Then cut-and-paste this into the Gemini chat window:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Tell me about the SDSS, the digital sky survey. How many objects did it measure? What was its sky coverage? What telescope did it use? I'm told it used drift scanning for the imaging survey, what is that? I'm also told that optical fibers were used for spectroscopy. How were the fibers positioned at sources of interest?*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Data File**\n",
        "\n",
        "You will be using the data file *A1942nohead.tsv*. It needs to be placed on your Google Drive in the same directory where your notebook is stored. You can fetch it from the course Canvas site. First put it on your local computer, then go to your Google Drive folder and use File Upload to send it there.\n",
        "\n",
        "We are going to use measurements of the apparent brightness of objects in a data file centered on RA=219.59 and DEC=+3.67. Let's do a quick review of astronomical magnitudes. Cut and paste this into the Gemini chat window:\n",
        "\n",
        "---\n",
        "\n",
        "*Please provide me with a quick summary of photometric measurements and the definition of astronomical magnutides. Ask me a couple of simple questions to ensure that I understand the flux ratio between objects that differ by 5 magnitudes, and the sense of whether brighter or fainter objects have larger magnutides. Pause to obtain a response to each question before proceeding.*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Expected Data Structure**\n",
        "- A structured table with:\n",
        "  - **RA, Dec** (coordinates)\n",
        "  - **Object ID**\n",
        "  - **PSF Magnitudes & Uncertainties** in *u,g,r,i*, and *z* bands\n",
        "  - **Petrossian magnitudes and Uncertainties (same bands).**\n",
        "\n",
        "### **Photometry of Point Sources and Extended Sources**\n",
        "\n",
        "As seen from a ground-based telescope, point sources (such as stars and galaxies that are so small they look like points) are blurred by turbulence in the atmospehre (what astronomers call 'seeing'). These point sources produce two-dimensional Gaussian flux distributions on the focal plane. That 2-d distribution is called the Point Spread Function (PSF). Adding up the flux within that PSF produces a measured quantity called the PSF magnitude, along with its associated uncertainty. The width of the PSF is usually called the Full Width at Half Maximum (FWHM), and typically subtends an angle of order 1 arc second.  \n",
        "\n",
        "Extended sources, like galaxies, produce smudges of light that are wider than the PSF FWHM. Astronomical codes that measure the brightness of these extended sources use some simple mathematical model that includes a parameterization of shape as well as flux.\n",
        "\n",
        "The apparent angular size of a galaxy is a rather complicated function of its distance from us. Since we don't know ahead of time how far away a galaxy is,  it's useful to define a measurement of the magnitude of a galaxy that is largely insensitive to the relationship between size and distance. 'Petrosian magnitudes' do this, by adding up the flux within a certain radius away from the center of the object.\n",
        "\n",
        "Cut and paste this prompt into the Gemini chat box:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Please give me a quick summary of how Petrosian magnitudes work, and why it's a good method for measuring the apparent magnitudes of galaxies. For a star, are the Petrosian and PSF magnitudes the same? What about for resolved galaxies? Also, remind me what a 'resolved' object means*\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "fPA_NB1dJMmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an illustration of the brightness vs. position in x and y on the focal plane of a telescope:"
      ],
      "metadata": {
        "id": "xAu147ELKupL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def gaussian_psf(x, y, amplitude, x0, y0, sigma_x, sigma_y):\n",
        "    \"\"\"\n",
        "    Compute a 2D Gaussian Point Spread Function (PSF).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        x-coordinates (can be a grid or 1D array).\n",
        "    y : array_like\n",
        "        y-coordinates (same shape as `x`).\n",
        "    amplitude : float\n",
        "        Peak value of the Gaussian (maximum flux).\n",
        "    x0 : float\n",
        "        Center position in the x-direction.\n",
        "    y0 : float\n",
        "        Center position in the y-direction.\n",
        "    sigma_x : float\n",
        "        Standard deviation (width) along the x-axis.\n",
        "    sigma_y : float\n",
        "        Standard deviation (width) along the y-axis.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    flux : ndarray\n",
        "        The flux values at each (x, y) position, with the same shape as `x` and `y`.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This function is commonly used to approximate the brightness distribution\n",
        "    of a star on a telescope’s focal plane.\n",
        "    \"\"\"\n",
        "    exponent = -((x - x0)**2 / (2 * sigma_x**2) + (y - y0)**2 / (2 * sigma_y**2))\n",
        "    return amplitude * np.exp(exponent)\n",
        "\n",
        "# Create a grid of x and y values (focal plane positions)\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = np.linspace(-5, 5, 100)\n",
        "x, y = np.meshgrid(x, y)\n",
        "\n",
        "# Define Gaussian PSF parameters\n",
        "amplitude = 1.0   # Peak flux\n",
        "x0, y0 = 0.0, 0.0 # Center position\n",
        "sigma_x = 1.0     # Width in x\n",
        "sigma_y = 1.0     # Width in y\n",
        "\n",
        "# Calculate the flux distribution on the focal plane\n",
        "flux = gaussian_psf(x, y, amplitude, x0, y0, sigma_x, sigma_y)\n",
        "\n",
        "# Plot the 3D surface of the flux distribution\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(x, y, flux, cmap='viridis')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('x (pixels)')\n",
        "ax.set_ylabel('y (pixels)')\n",
        "ax.set_zlabel('Flux (arbitrary units)')\n",
        "ax.set_title('Flux Distribution of a Star (Gaussian PSF)')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Id0z_LlORDRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I need to open my local Google drive and cd to the Colab notebooks folder\n",
        "# this is commented out to show a different method, left here as a fallback/mitigation!\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#%cd /content/drive/MyDrive/Colab\\ Notebooks\n"
      ],
      "metadata": {
        "id": "EaXWIDxWLcas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's a slightly different method to upload a data file.\n",
        "# Download the file A1942_nohead.tsv from Canvas onto your computer, probably into\n",
        "# your laptop's Downloads directory. Running this cell will allow you to designate that file for\n",
        "# upload into your Colab Notebooks directory on your remote Google Drive.\n",
        "\n",
        "# go ahead and run this, click on Choose Files and then find and select A1942_nohead.tsv for transfer.\n",
        "# the designation .tsv means the columns are separated by TAB characters, so it's a Tab Separated Value (tsv) file.\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "a71Wx34yRCPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I have a tab-delimited data file with the structure listed below. I want to read it into a pandas data frame called FullCatalog_df.\n",
        "# The name of the input file is A1942_nohead.tsv. Here are the first two lines:\n",
        "# _r      _RAJ2000        _DEJ2000        mode    cl      SDSS9   RA_ICRS DE_ICRS ObsDate Q       umag    e_umag  gmag    e_gmag  rmag    e_rmag  imag    e_imag  zmag    e_zmag        gpmag   e_gpmag gPmag   e_gPmag gdVrad  gdVell  rpmag   e_rpmag rPmag   e_rPmag rPrad   rdVell  ipmag   e_ipmag iPmag   e_iPmag zpmag   e_zpmag zPmag   e_zPmag\n",
        "# arcmin  deg     deg                             deg     deg     yr              mag     mag     mag     mag     mag     mag     mag     mag     mag     mag     mag     mag           mag     mag     arcsec  arcsec  mag     mag     mag     mag     arcsec  arcsec  mag     mag     mag     mag     mag     mag     mag     mag\n",
        "# ------- ------------    ------------    -       -       -------------------     ----------      ----------      ---------       -       ------  ------  ------  ------  ------        ------  ------  ------  ------  ------  ------  ------  ------  ------  ------- ------- ------  ------  ------  ------  ------- ------- ------  ------  ------  ------        ------  ------  ------  ------\n",
        "# 21.0543 182.91850900    +34.35843100    1       6       J121140.44+342130.3     182.918509      +34.358431      2004.2827       3       22.385   0.281  19.686   0.014  18.412         0.008  17.784   0.007  17.384   0.014  19.777   0.021  19.711   0.022     0.31   0.050 18.492   0.018  18.431   0.011     1.69   0.050 17.897   0.017  17.759   0.009        17.514   0.019  17.363   0.021\n",
        "# I need the column names to appear in the data frame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming 'A1942nohead.tsv' is in your current working directory\n",
        "# If not, specify the full path to the file\n",
        "\n",
        "# this line reads in the contents of the data file, and puts it into a \"Pandas\" data structure.\n",
        "# think of it as a large table with one row per object. The columns are listed above.\n",
        "# the next bit invokes the read.csv function, where csv means Comma Separated Values,\n",
        "# except we'll stipulate that the separator between fields is a Tab character instead of a comma.\n",
        "# also, the first line in the file contains the column headers so we tell it that as well.\n",
        "\n",
        "# the data are read into a data frame called FullCatalog_df.\n",
        "\n",
        "FullCatalog_df = pd.read_csv('A1942nohead.tsv', sep='\\t', header=[0])\n"
      ],
      "metadata": {
        "id": "byppJJNJJdie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of sources, and print that out."
      ],
      "metadata": {
        "id": "2pZdw8VPNIBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FullCount = len(FullCatalog_df)\n",
        "print(FullCount)\n",
        "\n",
        "# take a look at the first few rows of the data structure\n",
        "\n",
        "FullCatalog_df.head()\n"
      ],
      "metadata": {
        "id": "ZyYpm-6vYjGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In astronomy, we often describe how bright an object is using magnitudes rather than raw fluxes.\n",
        "\n",
        "Let’s walk through how flux relates to magnitude, what the zero point means, and how to correctly compute errors on magnitudes from flux errors."
      ],
      "metadata": {
        "id": "c5BCtSoyVkZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Magnitude–Flux Relationship\n",
        "\n",
        "The **apparent magnitude** $m$ of a source is related to its measured flux $F$ (in some band, here the *i*-band) through the formula:\n",
        "\n",
        "\n",
        "$m = -2.5 \\, \\log_{10}(F) + C$\n",
        "\n",
        "\n",
        "- $F$: the observed flux of the object.  \n",
        "- $C$: the **zero point**, a constant that sets the magnitude scale.  \n",
        "\n",
        "---\n",
        "\n",
        "### What is the Zero Point?\n",
        "\n",
        "The **zero point** ensures that the magnitude system matches a standard reference.  \n",
        "- Historically, the zero point was chosen so that bright stars like Vega have magnitude close to 0 in optical bands.  \n",
        "- In modern CCD-based surveys, the zero point comes from calibrations that convert instrumental flux counts into physical magnitudes.  \n",
        "\n",
        "---\n",
        "\n",
        "### Error Propagation: From Flux to Magnitude\n",
        "\n",
        "We also need the **uncertainty on the magnitude** $\\sigma_m$, given the flux error $\\sigma_F$.\n",
        "\n",
        "The general rule is:\n",
        "\n",
        "$\n",
        "\\sigma_m = \\left| \\frac{dm}{dF} \\right| \\, \\sigma_F\n",
        "$\n",
        "\n",
        "#### Step 1. Differentiate the magnitude formula:\n",
        "\n",
        "$\n",
        "m(F) = -2.5 \\log_{10}(F) + C\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{dm}{dF} = -\\frac{2.5}{\\ln(10)} \\cdot \\frac{1}{F}\n",
        "$\n",
        "\n",
        "#### Step 2. Apply error propagation:\n",
        "\n",
        "$\n",
        "\\sigma_m = \\frac{2.5}{\\ln(10)} \\cdot \\frac{\\sigma_F}{F}\n",
        "$\n",
        "\n",
        "Note that that $\\frac{2.5}{\\ln(10)} ≈ 1.0857$, which is pretty darn close to 1.0\n",
        "\n",
        "This shows that:\n",
        "- The **relative error in flux** ($\\sigma_F/F$) is what matters.  \n",
        "- For modest variations in flux, the magnitude difference is approximately equal to the fractional flux uncertainity. If sigma_m is 0.1, then fractional uncertainty is 10% and reciprocal of that, the signal to noise ratio, is 10.\n",
        "- Magnitude errors get larger for faint sources (small $F$) because dividing by $F$ amplifies uncertainties.  \n",
        "\n",
        "\n",
        "**For reasonable SNR values, the magniture uncertainty is approximately the inverse of the SNR.**\n"
      ],
      "metadata": {
        "id": "CMIU7rkfMHyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now add another plot, that looks at signal-to-noise ratio for galaxy photometry. For modest values of uncertainty, the uncertainty in magnitudes is roughly the fractional uncertainty in flux (see explanation above). So $\\sigma_m$ of 0.1 mag corresponds to a 10% flux uncertainty, or a signal to noise ratio of 10."
      ],
      "metadata": {
        "id": "w2OObPgIUt-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s only include objects that have a decent signal-to-noise ratio.\n",
        "The uncertainty in a magnitude measurement is (roughly) the fractional uncertainty in flux.\n",
        "We can start by making a histogram of the galaxy photometry uncertainties in the r-band cModel magnitude, `r_cModelMagErr`."
      ],
      "metadata": {
        "id": "3Rk53VV8paSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure with two subplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=False)\n",
        "\n",
        "# Linear scale\n",
        "axes[0].hist(FullCatalog_df[('e_rPmag')], bins=3000, edgecolor='black')\n",
        "axes[0].set_xlabel(r'$r_{\\rm cModel}$ uncertainty, $\\sigma_r$ (mag)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Histogram (Linear scale)')\n",
        "axes[0].set_xlim(0, 2)\n",
        "\n",
        "# Log scale\n",
        "axes[1].hist(FullCatalog_df[('e_rPmag')], bins=3000, edgecolor='black')\n",
        "axes[1].set_xlabel(r'$r_{\\rm cModel}$ uncertainty, $\\sigma_r$ (mag)')\n",
        "axes[1].set_ylabel('Frequency (log scale)')\n",
        "axes[1].set_title('Histogram (Log scale)')\n",
        "axes[1].set_xlim(0, 2)\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qz4CIOHvWcXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the median uncertainty\n",
        "print( np.median(FullCatalog_df['e_rPmag']))"
      ],
      "metadata": {
        "id": "5WM6U0HMdLoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So if we impose a cut for objects with SNR>=5, that correponds to $\\sigma_r$ <=0.2. It seems we'll lose about half the objects in the catalog. Let's make a subset of highSNR objects by imposing this cut.\n"
      ],
      "metadata": {
        "id": "QhcJOjqxXp6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma_mag_cut = 0.2\n",
        "\n",
        "# Apply SNR cut: keep objects with r-band cModel magnitude error <= sigma_mag_cut mag\n",
        "HighSNR_df = FullCatalog_df[FullCatalog_df['e_rPmag'] <= sigma_mag_cut]\n",
        "\n",
        "# Compare the number of rows\n",
        "original_count = len(FullCatalog_df)\n",
        "highsnr_count = len(HighSNR_df)\n",
        "\n",
        "print(f\"Original DataFrame has {original_count} rows.\")\n",
        "print(f\"HighSNR DataFrame has {highsnr_count} rows.\")\n"
      ],
      "metadata": {
        "id": "4-wM4k8rXoOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Star-galaxy separation"
      ],
      "metadata": {
        "id": "KIkIa8G-Unz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next task is to separate stars from galaxies. There are a couple of ways to do this. One is to compare PSF magnitudes, which does photometry assuming a 2d Gaussian point source, to Petrosian magnitudes that allow for a broader shape. A standard metric is to compute (PSF magnitude - Petrosian magnitude), and for galaxies we'll exclude cases where that is close to zero. A standard metric is to compute (`drmag` = PSF magnitude – Petrossian magnitude). Stars have values close to zero, while galaxies deviate from zero because of their extended shapes."
      ],
      "metadata": {
        "id": "ql419UYCftya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make sure drmag is defined with the new quantities\n",
        "HighSNR_df = HighSNR_df.copy()\n",
        "HighSNR_df['drmag'] = HighSNR_df['rpmag'] - HighSNR_df['rPmag']\n",
        "\n",
        "# Create figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True)\n",
        "\n",
        "# Zoomed-in view (typical star/galaxy separation)\n",
        "axes[0].plot(HighSNR_df['rPmag'], HighSNR_df['drmag'], 'o', markersize=2)\n",
        "axes[0].set_xlabel('rPmag')\n",
        "axes[0].set_ylabel('drmag = rpmag – rPmag')\n",
        "axes[0].set_title('Zoomed-in: Star/Galaxy Separation')\n",
        "axes[0].set_ylim(-0.2, 1)\n",
        "\n",
        "# Full range view (to catch outliers)\n",
        "axes[1].plot(HighSNR_df['rPmag'], HighSNR_df['drmag'], 'o', markersize=2)\n",
        "axes[1].set_xlabel('rPmag')\n",
        "axes[1].set_title('Larger drmag range')\n",
        "# axes[1].set_ylim(-1, 8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EUHcaBz-UkKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In any selection there is a tradoff between purity and completeness.**\n",
        "\n",
        "Let's allow a few stars to leak in with a fairly loose cut, but always keep this tradeoff in mind."
      ],
      "metadata": {
        "id": "0ReHXPqsjJrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this distribution to discriminate between stars and galaxies. The PSFMag values assume a Gaussian point source, while the Petrossian magnitude values are more flexible and add up flux in a larger aperture. If these two are the same, it's likely to be a star.\n",
        "\n",
        "Sorry about the lousy notation (blame SDSS!) but remember that rpmag is PSF magnitude and rPmag is a Petrossian magnitude (name for Vahe Petrossian of Stanford).\n",
        "\n",
        "Let's define a `star_galaxy_cut` of `0.1` in `drmag` to separate the stars and the galaxies, and the visualize with different colors and symbols de stellar locus.\n",
        "\n"
      ],
      "metadata": {
        "id": "NVeiMfdlsGY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "star_galaxy_cut = 0.1\n",
        "\n",
        "# Select galaxies: those with |drmag| > star_galaxy_cut\n",
        "\n",
        "#HighSNRGalaxies = HighSNR_df[np.abs(HighSNR_df['drmag']) > star_galaxy_cut].copy()\n",
        "\n",
        "stars = HighSNR_df[HighSNR_df['drmag'].abs() < star_galaxy_cut]\n",
        "galaxies = HighSNR_df[HighSNR_df['drmag'].abs() >= star_galaxy_cut]\n",
        "\n",
        "print(f\"HighSNR sample: {len(HighSNR_df)} objects\")\n",
        "print(f\"High SNR Galaxies: {len(galaxies)} objects\")\n",
        "galaxies.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "q5QVWyiHgrBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot stars\n",
        "plt.plot(stars['rPmag'], stars['drmag'],\n",
        "         'o', color='blue', markersize=2, label=f'Stars (|drmag| < {star_galaxy_cut})')\n",
        "\n",
        "# Plot galaxies\n",
        "plt.plot(galaxies['rPmag'], galaxies['drmag'],\n",
        "         's', color='red', markersize=3, label=f'Galaxies (|drmag| ≥ {star_galaxy_cut})')\n",
        "\n",
        "# Add horizontal dashed lines for the separation threshold\n",
        "# plt.axhline(0.02, color='gray', linestyle='--', linewidth=1)\n",
        "# plt.axhline(-0.02, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('rPmag')\n",
        "plt.ylabel('drmag = rpmag – rPmag')\n",
        "plt.title('Star/Galaxy Separation in High SNR sample')\n",
        "\n",
        "# Legend\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GxZnbDTiYHHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completeness Determination\n",
        "There are more faint sources than bright ones, for a variety of reasons. This means we should expect a histogram of the number of sources vs. their magnitude to continuously increase. Let's make a plot of the number of sources vs. magnitude, as well as a cumulative plot that takes the integral of number of sources down to some faintness limit $f$, as a function of $f$."
      ],
      "metadata": {
        "id": "psmNM8n0E1Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Histogram of rPmag magnitudes for galaxies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(galaxies['rPmag'], bins=40, edgecolor='black')\n",
        "plt.xlabel(r'$r_{P}$ (mag)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of rPmag Magnitudes (Galaxies)')\n",
        "plt.show()\n",
        "\n",
        "# Cumulative counts plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "values, base = np.histogram(galaxies['rPmag'], bins=40)\n",
        "\n",
        "cumulative = np.cumsum(values)\n",
        "plt.plot(base[:-1], cumulative, 'ko-')\n",
        "plt.xlabel(r'$r_{P}$ (mag)')\n",
        "plt.ylabel('Cumulative Count')\n",
        "plt.title('Cumulative Count Plot of rPmag Magnitudes (Galaxies)')\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D3orQMEkOLoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Take a look at the two plots produced above. Is there a threshold magnitude fainter than which there aren't many objects in the catalog of high-SNR galaxies? Remember that larger numbers correspond to fainter objects!  Why might this happen?\n",
        "\n",
        "<font color=\"red\"> **your input needed here** </font>\n",
        "\n",
        "The magnitude where the cumulative counts flatten out is the 'magnitude limit' of the catalog.\n",
        "\n",
        "It's important to remember that the completness of the catalog depends on the distance to the kind of object in question, and that we're plotting *apparent* magnitudes, not *absolute* magnitudes. What's the difference?\n",
        "\n",
        "<font color=\"red\"> **your input needed here** </font>\n",
        "\n"
      ],
      "metadata": {
        "id": "TyxH301lGzI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do we live in a static and Euclidean universe?\n",
        "\n",
        "Imagine the Universe was static and Euclidean, and that there was only one kind of galaxy with a single brightness. The number of objects within a sphere of radius R grows as R$^3$. But the ones farther away are fainter. If this is true for one single kind of galaxy, it's also true for a mixture of types.\n",
        "\n",
        "Cosmologists like to produce plots of 'Number Counts', which is the number of observed objects brighter than some magnitude $m$ vs. $m$.\n",
        "We usually show this with a logarithmic y axis.\n",
        "\n",
        "Cut and paste this prompt into the Gemini AI chat window:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Prompt**: *Give me an explanation for the predicted value of the slope of the number counts of galaxies as a function of magnitude, for a static Euclidean Universe.*\n",
        "\n",
        "---\n",
        "\n",
        "What's the predicted slope for a static Euclidean universe?\n",
        "\n",
        "<font color=\"red\"> **your input needed here** </font>\n",
        "\n",
        "\n",
        "We can compare our observed number count slope with this prediction.\n",
        "The cell below makes a fit to the number count plot for the high-SNR galaxies we selected.\n",
        "\n",
        "Remember you can always select the cell and then choose Explain Code from the menu under the three dots on the right of the icon panel!\n",
        "\n"
      ],
      "metadata": {
        "id": "j6JjAjyfh_bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def power_law(m, a, b):\n",
        "    \"\"\"\n",
        "    Power-law model for galaxy number counts in log-log space.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : array_like\n",
        "        Magnitudes (e.g., rPmag).\n",
        "    a : float\n",
        "        Intercept term (log10 normalization).\n",
        "    b : float\n",
        "        Slope of the relation (log10 counts vs. magnitude).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    log_counts : ndarray\n",
        "        Predicted log10 of cumulative counts for each magnitude.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This model assumes:\n",
        "        log10(N) = a + b * m\n",
        "    where N is the cumulative number of galaxies brighter than magnitude m.\n",
        "    \"\"\"\n",
        "    return a + b * m\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Step 1: Set magnitude range for fitting\n",
        "# ---------------------------------------------------------------------\n",
        "mag_min = 15.0      # lower limit of magnitudes to include in the fit\n",
        "mag_max = 20.5      # upper limit of magnitudes to include in the fit\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Step 2: Build cumulative counts of galaxy magnitudes\n",
        "# ---------------------------------------------------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Histogram of magnitudes (30 bins by default)\n",
        "values, base = np.histogram(galaxies['rPmag'], bins=30)\n",
        "\n",
        "# Compute cumulative counts (number of galaxies brighter than each bin edge)\n",
        "cumulative = np.cumsum(values)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Step 3: Select data within the chosen magnitude range for fitting\n",
        "# ---------------------------------------------------------------------\n",
        "mask = (base[:-1] >= mag_min) & (base[:-1] <= mag_max)\n",
        "mag_fit = base[:-1][mask]\n",
        "counts_fit = cumulative[mask]\n",
        "\n",
        "# Avoid log10(0) by keeping only strictly positive counts\n",
        "counts_fit = counts_fit[counts_fit > 0]\n",
        "mag_fit = mag_fit[:len(counts_fit)]  # Ensure arrays have same length\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Step 4: Perform log-log fit\n",
        "# ---------------------------------------------------------------------\n",
        "log_counts_fit = np.log10(counts_fit)               # take log10 of counts\n",
        "params, covariance = curve_fit(power_law, mag_fit, log_counts_fit)\n",
        "a_fit, b_fit = params\n",
        "slope_uncertainty = np.sqrt(np.diag(covariance))[1]  # uncertainty in slope b\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Step 5: Plot data and fit\n",
        "# ---------------------------------------------------------------------\n",
        "# Plot cumulative data\n",
        "plt.plot(base[:-1], cumulative, 'co-', label=\"Data\")\n",
        "plt.yscale('log')  # log scale highlights power-law behavior\n",
        "\n",
        "plt.xlabel(r'$r_{P}$ (mag)')\n",
        "plt.ylabel('Cumulative Count')\n",
        "plt.title('Cumulative Count Plot of rPmag Magnitudes')\n",
        "\n",
        "# Plot fitted model\n",
        "mag_range = np.linspace(mag_min, mag_max, 100)\n",
        "plt.plot(mag_range, 10**power_law(mag_range, *params),\n",
        "         'r--', label=f'Fit: slope = {b_fit:.3f} ± {slope_uncertainty:.3f}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Step 6: Print slope (b) of the fitted relation\n",
        "# ---------------------------------------------------------------------\n",
        "print(f\"Fitted slope: {b_fit:.3f} ± {slope_uncertainty:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nizilJvCa8WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Comparison: Confronting a Prediction with Data ##\n",
        "OK, we have a measurement of the number count slope, and we have a prediction. How do they compare?? Is there a statistically significant difference between the prediction and the measurement? By how many standard deviations do they differ? The cell below will help make that calculation. If we have two quantities $A$ and $B$ with associated uncertainties $\\sigma_A$ and $\\sigma_B$, their difference has an uncertainty of $\\sigma_\\Delta=\\sqrt{\\sigma_A^2 + \\sigma_B^2}$. The statistical significance of the difference, in units of standard deviations, is then |A-B|/$\\sigma_\\Delta$.\n",
        "\n",
        "How do we interpret this? Well... in the absence of better information we make an assumption of Gaussian statistics, so that roughly 68% of the time we should get less than one sigma of difference. A three sigma discrepancy is only supposed to happen by chance less than 1% of the time, and 5 standard deviations is considered iron-clad evidence for a discrepancy.\n",
        "\n",
        "Do take note, however, that something like 2 million technical papers are published per year.\n",
        "\n",
        "**That means some unlucky scientist somewhere is gloating or fretting about a one-in-a-million statistical result, but they're just the victom of bad luck.**\n",
        "\n",
        "Here's a wallet card version:\n",
        "\n",
        "| Number of Sigmas | How Frequent? | What to Bet  |\n",
        "|---|---|---|\n",
        "| 1 | 32% | A Coffee |\n",
        "| 2 | 4.5% |A Lunch |\n",
        "| 3 | 0.27% | A (cheap) Car |\n",
        "| 5 | 0.000057% | A House |\n",
        "| 10| 1.52 x 10$^{-23}$% |Your Entire Wealth |\n",
        "\n",
        "I have an entire rant about why this is all misleading, however. Here is the terse version:\n",
        "\n",
        "1. The world isn't Gaussian. That's a just a model, and it's wrong.\n",
        "2. Most experiments underestimate their uncertainties. A factor of two underestimate has an **exponential** impact on implied significance. So beware the likeilihoods shown above.\n",
        "3. Especially in cosmology, measurments suffer from sources of systematic (rather than statsitical) errors and those are certainly NOT Gaussian, and are very hard to quantify.\n"
      ],
      "metadata": {
        "id": "jjS6KwUB5ty-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute significance of number count slope difference. The prediction for a static Euclidean Universe is 0.6\n",
        "# We will take that as a perfect value, with no uncertainty. So the standard deviation of the difference\n",
        "# is just the measurment uncertainty that was produced in the fit above.\n",
        "\n",
        "\n",
        "Nsigma=np.abs(b_fit-0.6)/slope_uncertainty\n",
        "print(f\"Slope discrepancy significance is {Nsigma:.1f} sigma\")"
      ],
      "metadata": {
        "id": "hSi--pyv9F2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion ##\n",
        "\n",
        "Is there a significant discrepancy here, or not?\n",
        "\n",
        "Let's pause and consider what this means. Think of some reasons that could give rise to this result. Is the model that made the prediction ruled out? What might affect the measurements? Think hard about what we're measuring and what inplicit assumptions are being made.\n",
        "\n",
        "Type in your responses here:\n",
        "\n",
        "<font color=\"red\"> **your input needed here** </font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "505gxPwn9xxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgements\n",
        "\n",
        "List the resources you used for this exercise, including any use of AI tools.\n",
        "\n",
        "<font color=\"red\"> **your input needed here** </font>"
      ],
      "metadata": {
        "id": "ljYIcRgVn12E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **References** ###\n",
        "\n",
        "\n",
        "Star-galaxy separation:\n",
        "https://outerspace.stsci.edu/display/PANSTARRS/How+to+separate+stars+and+galaxies\n",
        "\n"
      ],
      "metadata": {
        "id": "UF-70FD9cXEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is supposed to generate a PDF file of this on Google Drive.\n",
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('ClusterLRG-Rubin-Part1-2025SEP25.ipynb')"
      ],
      "metadata": {
        "id": "FwUd-SX4wuUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}